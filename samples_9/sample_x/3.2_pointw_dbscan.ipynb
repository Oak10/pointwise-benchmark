{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Reliability: Density, Clustering - DBSCAN\n",
    "\n",
    "## Workflow:\n",
    "- Perform grid search over `eps` and `min_samples` \n",
    "  - Select the parameters that maximize the number of clusters\n",
    "- Train DBSCAN\n",
    "- Workflow Reliability:\n",
    "  - Determines whether the new instance is part of a cluster (eps distance to any core point in the cluster)\n",
    "  - Identifies which cluster the new instance would belong to based on its nearest core point\n",
    "  - Calculates the size of the identified cluster \n",
    "  - Compute reliability:\n",
    "    - reliability=  cluster_size / `minimum_cluster_units`\n",
    "      - `minimum_cluster_units` <- param \n",
    "      - If the cluster size is larger than the threshold (minimum cluster units): reliability = 1\n",
    "      - If the new instance is not part of any cluster: reliability = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.reliability_visualizer import ReliabilityVisualizer\n",
    "\n",
    "###########################################################\n",
    "# Load Classifier and Training Data\n",
    "###########################################################\n",
    "pipeline = joblib.load('pipeline_classifier.pkl')\n",
    "training_data = pd.read_csv(\"./data_train.csv\")\n",
    "\n",
    "# Map labels and preprocess training data\n",
    "y_train = training_data['SOURCE'].map({'out': 0, 'in': 1})\n",
    "X_train = training_data.drop(columns=['SOURCE'])\n",
    "\n",
    "# Preprocess training data using the loaded pipeline\n",
    "X_train_preprocessed = pipeline.named_steps['preprocessor'].transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Grid search DBSCAN Parameters\n",
    "###########################################################\n",
    "\n",
    "eps_values = np.linspace(0.01, 0.5, 10)\n",
    "min_samples_values = range(2, 10, 2)\n",
    "\n",
    "results = []\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan.fit(X_train_preprocessed)\n",
    "        \n",
    "        labels = dbscan.labels_\n",
    "        n_clusters = len(set(labels) - {-1})\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        if n_clusters > 0:\n",
    "            cluster_sizes = [np.sum(labels == label) for label in set(labels) if label != -1]\n",
    "            max_samples = max(cluster_sizes)\n",
    "        else:\n",
    "            max_samples = 0\n",
    "        \n",
    "        results.append({\n",
    "            \"eps\": eps,\n",
    "            \"min_samples\": min_samples,\n",
    "            \"n_clusters\": n_clusters,\n",
    "            \"n_noise\": n_noise,\n",
    "            \"max_samples\": max_samples\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "best_params = results_df.loc[results_df['n_clusters'].idxmax()]\n",
    "best_eps = best_params['eps']\n",
    "best_min_samples = best_params['min_samples']\n",
    "best_max_samples = best_params['max_samples']\n",
    "\n",
    "print(\"Best Parameters to Maximize Clusters:\")\n",
    "print(f\"eps: {best_eps}\")\n",
    "print(f\"min_samples: {best_min_samples}\")\n",
    "print(f\"max_samples: {best_max_samples}\")\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Train DBSCAN\n",
    "###########################################################\n",
    "eps_value = best_eps\n",
    "print(f\"Best min samp: {int(best_min_samples)}\") \n",
    "min_samples_value = int(best_min_samples)\n",
    "\n",
    "dbscan = DBSCAN(eps=eps_value, min_samples=min_samples_value)\n",
    "dbscan.fit(X_train_preprocessed)\n",
    "\n",
    "core_samples_mask = dbscan.labels_ != -1  # Exclude noise (-1)\n",
    "core_points = X_train_preprocessed[core_samples_mask]\n",
    "core_labels = dbscan.labels_[core_samples_mask]\n",
    "\n",
    "print(f\"Number of clusters (excluding noise): {len(set(core_labels))}\")\n",
    "print(f\"Number of core points: {len(core_points)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Reliability Calculation with DBSCAN\n",
    "###########################################################\n",
    "def calculate_reliability_with_dbscan(new_instance, core_points, core_labels, dbscan, min_cluster_units):\n",
    "    \"\"\"\n",
    "    Calculate reliability of a new instance based on DBSCAN clusters.\n",
    "\n",
    "    Parameters:\n",
    "    - new_instance: Preprocessed new instance.\n",
    "    - core_points: Core points identified by DBSCAN.\n",
    "    - core_labels: Cluster labels for core points.\n",
    "    - dbscan (DBSCAN): Trained DBSCAN model on preprocessed training data.\n",
    "    - min_cluster_units (int): Minimum number of points required for full reliability.\n",
    "\n",
    "    Returns:\n",
    "    - reliability_score: Reliability score based on cluster size.\n",
    "    - in_cluster: Boolean indicating if the instance is part of a cluster.\n",
    "    \"\"\"\n",
    "    # Find the nearest core point and its distance\n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(core_points)\n",
    "    distance_to_core_point, nearest_core_idx = nbrs.kneighbors(new_instance.reshape(1, -1))\n",
    "    nearest_core_idx = nearest_core_idx[0][0]  # Index of the nearest core point\n",
    "    nearest_label = core_labels[nearest_core_idx]  # Cluster label of the nearest core point\n",
    "\n",
    "    # Check if the new instance is in a cluster\n",
    "    if nearest_label == -1 or distance_to_core_point[0][0] > dbscan.eps:\n",
    "        return 0, False\n",
    "\n",
    "    # Size of Cluster\n",
    "    cluster_size = np.sum(core_labels == nearest_label)\n",
    "\n",
    "    # Reliability\n",
    "    reliability_score = min(cluster_size / min_cluster_units, 1.0)\n",
    "    return reliability_score, True\n",
    "\n",
    "\n",
    "def classify_and_check_reliability_dbscan(new_data, pipeline, core_points, core_labels, dbscan, min_cluster_units):\n",
    "    \"\"\"\n",
    "    Predicts the class, determines cluster membership, and calculates reliability.\n",
    "\n",
    "    Parameters:\n",
    "    - new_data (pd.DataFrame): DataFrame with the same columns as training data.\n",
    "    - pipeline (Pipeline): The loaded pipeline for preprocessing and classification.\n",
    "    - core_points: Core points identified by DBSCAN.\n",
    "    - core_labels: Cluster labels for core points.\n",
    "    - dbscan: Trained DBSCAN model.\n",
    "    - min_cluster_units (int): Minimum number of points required for full reliability.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Contains prediction, cluster membership, and reliability score.\n",
    "    \"\"\"\n",
    "\n",
    "    new_data_preprocessed = pipeline.named_steps['preprocessor'].transform(new_data)\n",
    "\n",
    "    predicted_label = pipeline.named_steps['classifier'].predict(new_data_preprocessed)[0]\n",
    "\n",
    "    # Calculate reliability\n",
    "    reliability_score, in_cluster = calculate_reliability_with_dbscan(\n",
    "        new_data_preprocessed[0], core_points, core_labels, dbscan, min_cluster_units\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"predicted_label\": predicted_label,\n",
    "        \"reliability_score\": reliability_score,\n",
    "        \"in_cluster\": in_cluster\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Load Validation Data and Test\n",
    "###########################################################\n",
    "validation_data = pd.read_csv(\"./data_validation.csv\")\n",
    "validation_data['SOURCE'] = validation_data['SOURCE'].map({'out': 0, 'in': 1})\n",
    "results = []\n",
    "\n",
    "################\n",
    "#### Adjust\n",
    "min_cluster_units = 9  # Minimum points for full reliability\n",
    "\n",
    "for _, instance in validation_data.iterrows():\n",
    "    true_label = instance['SOURCE']\n",
    "    instance_df = pd.DataFrame([instance.drop(labels=['SOURCE'])])\n",
    "    result = classify_and_check_reliability_dbscan(\n",
    "        instance_df, pipeline, core_points, core_labels, dbscan, min_cluster_units\n",
    "    )\n",
    "    result['true_label'] = true_label\n",
    "    results.append(result)\n",
    "\n",
    "# Filter results where reliability_score > 0\n",
    "filtered_results = [result for result in results if result['reliability_score'] > 0]\n",
    "# filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Results and Visualization\n",
    "###########################################################\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "visualizer = ReliabilityVisualizer(results_df=results_df)\n",
    "visualizer.plot_reliability_scores((20, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_table_10_df = visualizer.get_reliability_table_10()\n",
    "print(reliability_table_10_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_table_graph(reliability_table_10_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_population_and_error_rate_by_class(reliability_table_10_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_tsne_with_reliability(\n",
    "    X_train_preprocessed, validation_data, y_train, pipeline\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
